When you hear people talk about automated testing, the subject of code coverage typically comes up as well.  Code coverage is a metric which indicates how much of your production code is <strong>executed</strong> while your automated tests are running.  A lot of people think that high code coverage is a good thing, because it means that a high percentage of their production code is being executed by their tests.  That's gotta be a good thing, right?

Well, not necessarily.  You can very easily write bad tests which execute most, or even all of your production code, while the tests themselves probably don't cover everything that really needs to be covered.  Even if you're a disciplined developer who is very careful not to make mistakes, the odds that your code will contain bugs that your tests didn't catch are still relatively likely.  For those of you who write automated tests (either test-first or test-after): how often has it happened to you that a bug was discovered later on for some edge case that you didn't think of when writing the production code or the test code?  No matter how good you are, this has happened to you on more than one occasion.  And it will happen again in the future.  

Here's the important question though: how frequently does it happen that bugs are discovered in pieces of code which have high coverage during the test-runs?  It might not happen every day, it might not happen every week but it definitely happens occasionally and in some cases even frequently.  What exactly does that tell you about the value of a metric such as code coverage?  You might think that if this situation comes up, you don't have proper tests in place but that's not necessarily the case.  You can have an incredibly good test fixture (or several) for a certain piece of code but if you and/or your teammates didn't happen to think of a certain edge-case or even something that might be obvious, then you didn't write tests for that.  Even though the build report might indicate 100% coverage for that piece of code.

I've always thought that <strong>code coverage is only meaningful when it's low</strong>.  Low code coverage can at least definitively tell you that certain parts of code are never executed during test-runs and are thus completely untested. However, it will never definitively tell you which parts of code (or how much of it) are guaranteed to be covered <strong>correctly</strong> by your tests.  As such, I would say that code coverage can only be used as an indication that something is wrong, but never as an indication that something is good/right/correct/great.