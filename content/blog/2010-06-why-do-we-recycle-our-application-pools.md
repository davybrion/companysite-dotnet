I’ve never understood why IIS by default configures newly created application pools to recycle every 1740 minutes. That means that by default, our ASP.NET applications are restarted every 29 hours. And for what? Are we, the .NET community, so bad that we really can’t write code that can keep running without problems for more than 29 hours? Well, considering the overall lack of knowledge regarding memory management in .NET, i actually wouldn’t be surprised if that turned out to be the case for a very large portion of .NET development teams.

And indeed, if you browse the web to find the reason why application pools are configured to recycle automatically periodically, you’ll be hard pressed to find a reasonable answer that doesn’t pertain to memory issues. It’s like the community in general has pretty much accepted the fact that our web applications (or service layers hosted in IIS) will need to be recycled to avoid memory problems. To make things worse, i’ve frequently seen people discuss workarounds on how to keep things working properly after an application pool recycle in mailinglists for various projects. And again i ask myself: why?

I’ve always been of the opinion that if your code requires periodic restarts to keep working correctly, then something is *clearly wrong*. There is a bug in your code somewhere and you need to fix *that*, instead of restarting the process occasionally to make the problem ‘go away’. Guess what, it’s not going away, and if the load on your application increases, the problem will only pop up more frequently. So what are you going to do about that? Make the application pool recycle even more frequently? Put in another server to share the load? Perhaps we should focus more on fixing the actual code. You’re leaking resources somewhere, and *that* is what you need to fix.

In my company, it’s just standard procedure to disable application pool recycling on all application pools. Our applications simply *have* to keep working without application pool recycles. It’s that simple. If we run into a memory leak that causes problems, we focus on the actual problem and we fix it. We’ve ran into plenty of memory leaks (though it hasn’t really happened a lot in the past year, guess why?) and we’ve fixed each and every single one of them that we’ve ran into. Side note: i’m talking about .NET code that is running on the server. While i’m pretty sure that we do better at memory management in our Silverlight clients than most other Silverlight-using-shops do, you just never know for sure with Silverlight, partly because there are so many memory leaks in Silveright *itself*. But for .NET code running on a server, a memory leak is simply inexcusable in my opinion. I mean, it can happen to the best of us (i’m pretty much responsible for most of ours to be honest), but it’s not about who introduced the memory leak. It’s all about how you *deal with it*. And recycling the application pool periodically is *not* dealing with it, it’s living in denial.

The only viable reason to recycle application pools that i can think of is in the situation of a shared hosting provider. If you’re offering shared hosting, it only makes sense to recycle application pools occasionally since it’s quite realistic to assume that most of those application pools will not be in use constantly. If a pool is recycled, it will not consume any resources until the next request for an application in that pool is received. In the context of shared hosting, that actually makes sense. In the context of real applications, that makes little sense unless you’re hosting a lot of them and plenty of them are hardly being used. Still, is that really a good enough reason to enabling application pool recycling by default? I obviously don’t think so.

In my opinion, we, as a community, really need to start focusing more on [memory management in .NET](/blog/2008/08/net-memory-management/) and on making sure that our applications can *keep running without problems.*