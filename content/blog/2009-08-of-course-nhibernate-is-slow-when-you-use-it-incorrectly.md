Just saw the following <a href="http://gregdoesit.com/2009/08/nhibernate-vs-entity-framework-a-performance-test/">post</a> where the performance of NHibernate and Entity Framework is compared for a couple of different operations.  Spoiler alert: NHibernate loses.  As it typically does in these kinds of 'benchmarks' or 'comparisons' that seem to pop up frequently lately.

For some reason, a lot of people seem to think that opening an NHibernate session and performing thousands of operations is a valid use case.  It's not.  Far from it actually.  And with all of the features that NHibernate offers, it can't possibly perform well in such a scenario.  See, an NHibernate session is a <a href="http://martinfowler.com/eaaCatalog/unitOfWork.html">unit of work</a>.  A unit of work is a business transaction which is typically short and small, but it should never be something huge.  Your DBA probably won't appreciate huge database transactions on the database either.

Whenever you load an object through NHibernate, it will be tracked by the session that loaded it.  That means that the NHibernate session keeps a reference to it, and performs a series of checks on it periodically, depending on what you're doing and some configuration settings such as the FlushMode.  For instance, suppose you've loaded a hundred different entities in one session.  If the FlushMode is set to automatic, it means that NHibernate will perform a dirty check for each entity associated with the session before each query is executed.  The more entity instances you've loaded, the longer this takes (obviously).  If you take this to an extreme level, like loading thousands of entities like most of these 'benchmarks' do, performance will naturally be horrible.

Each entity instance is also stored in the first level (or session level) cache.  That means that whenever you retrieve a row from the database, NHibernate will check if an instance of that row already exists in the first level cache.  Again, the more instances you've loaded, the larger the overhead of this will be.   There are also a lot of possible extension points where you can plug in custom logic.  Again, there is a very minor cost that comes with this extensibility and as you can expect, that minor cost can add up to something much more noticable once you start dealing with an unreasonably large number of instances in your session.

Always keep in mind that an ORM (and this goes for every ORM) is most suitable for <a href="http://en.wikipedia.org/wiki/OLTP">OLTP</a>.  Using an ORM for batch processing jobs or large data processing operations in general is simply put a bad idea.  And they will never perform as well as other solutions in these scenarios.  So please don't bother even benchmarking ORM performance in non OLTP usage because it quite simply doesn't make sense to do so, and the results will be completely untrustworthy anyway.

An ORM can offer you nice performance gains in OLTP scenarios simply by trying to minimize database connectivity, minimizing the number of database operations, and relatively sane caching usage.  Unfortunately, these are aspects that are never tested in these 'benchmarks' or 'comparisons'.